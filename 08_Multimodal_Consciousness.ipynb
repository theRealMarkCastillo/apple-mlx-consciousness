{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "from conscious_chatbot import ConsciousChatbot\n",
    "import os\n",
    "\n",
    "# Initialize the upgraded agent\n",
    "bot = ConsciousChatbot(use_heterogeneous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930f1e3",
   "metadata": {},
   "source": [
    "## Experiment 1: Biological NLP (Text Only)\n",
    "We test the agent's ability to understand and respond using only Sparse Distributed Representations and Associative Memory. No LLM is involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bot.chat(\"Who are you?\")\n",
    "print(f\"Bot: {response['response']}\")\n",
    "print(f\"Thought Process: {response['thought_process']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcfcafe",
   "metadata": {},
   "source": [
    "## Experiment 2: Multimodal Perception (Vision + Text)\n",
    "We simulate the agent \"seeing\" an image while conversing. The **Visual Cortex** processes the image, and the **Multi-Modal Fuser** combines it with the text input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c0d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy image (random noise) to simulate visual input\n",
    "# In a real scenario, this would be a path to a .jpg or .png\n",
    "dummy_image_path = \"test_image.png\"\n",
    "\n",
    "# Create a random noise image file for the demo\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "rand_im = np.random.randint(0, 255, (64, 64, 3), dtype=np.uint8)\n",
    "im = Image.fromarray(rand_im)\n",
    "im.save(dummy_image_path)\n",
    "\n",
    "print(f\"Created simulated visual input: {dummy_image_path}\")\n",
    "\n",
    "# Chat with visual context\n",
    "response = bot.chat(\"look at this image\", image_path=dummy_image_path)\n",
    "\n",
    "print(f\"Bot: {response['response']}\")\n",
    "print(f\"Thought Process: {response['thought_process']}\")\n",
    "\n",
    "# Cleanup\n",
    "if os.path.exists(dummy_image_path):\n",
    "    os.remove(dummy_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714054cf",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We have successfully demonstrated a **non-transformer, hardware-aware, multimodal cognitive architecture**.\n",
    "- **Input**: Text (SDRs) + Vision (CNN).\n",
    "- **Processing**: Heterogeneous (NPU + GPU).\n",
    "- **Consciousness**: Global Workspace Theory.\n",
    "- **Output**: Associative Retrieval."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
